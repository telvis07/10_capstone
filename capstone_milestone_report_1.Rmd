---
title: "Using text analysis techniques to build a predictive text model"
author: "Telvis Calhoun"
date: "June 10, 2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default

---

## Executive Summary

[Swiftkey](https://swiftkey.com/en) is a text messaging application that predicts the next word as you type a message. When the user types: `"I went to the "` : the application presents three options for what the next word might be. For example, the three words might be `gym, store, restaurant`.

In this project, we use `R` text mining tools to build a application that predicts the next word entered into an [shiny application](http://shiny.rstudio.com/). We use data scraped from blogs, twitter and news sites to build a language model to perform prediction. We predict the next word by calculating the [maximum likelihood estimate](https://en.wikipedia.org/wiki/Maximum_likelihood) (MLE) based on the language model.

### TL;DR

In a nutshell, here's a summary of the data analysis performed in this report.

1. Load the raw data.
2. Extract a 1% subsample of the data.
3. Preprocess the data to remove stopword, convert to lowercase and other tasks
4. Generate 2-grams, 3-grams and 4-grams.
5. Build a language model where a ngram (i.e. phrase) is a path for a `search node` in a tree.
6. Calculate the maximum likelihood estimate (MLE) using the children of the `search node`.


```{r, echo=FALSE,message=FALSE}
library(knitr)
library(dplyr)
library(tm)
library(RWeka)
library(xtable)

source("analysis.R")
source("explore.R")
source("modeling.R")
source("sample_data.R")
```

## Complete Dataset
The [Data Science Capstone Course](https://www.coursera.org/learn/data-science-project/home/welcome) provided text data from 3 data sources: `blogs`, `twitter` and `news`. The table below shows the number of lines from data source.

``` {r, echo=FALSE,cache=TRUE}
total_counts_df <- data.frame(
  filename = c(
    "en_US.blogs.txt",
    "en_US.news.txt",
    "en_US.twitter.txt",
    "Total"
  ),
  line_count = c(
    899288,
    1010242,
    2360148,
    4269678
  )
)
# $ wc -l data/final/en_US/*.txt
#   899288 data/final/en_US/en_US.blogs.txt
#  1010242 data/final/en_US/en_US.news.txt
#  2360148 data/final/en_US/en_US.twitter.txt
#  4269678 total
```


```{r echo=FALSE, fig.align='center', fig.cap="Raw Data Line Counts"}
kable(total_counts_df, 
      row.names=TRUE, 
      format = "markdown", 
      caption="Table 1: Raw Data Line Counts")
```

## Processing Unstructured Text for Analysis
We use [a framework for text mining applications within R](https://cran.r-project.org/web/packages/tm/index.html) to transform the unstructured text to a structured document-by-term matrix format required for analysis. The first step is "clean" the text prior with a series of text processing functions. We use the following preprocessing steps.

- Remove all Punctuation
- Remove all Numbers
- Convert all words to Lowercase
- Remove English Stopwords
- Strip extra whitespace
- Remove Profanity

Second, we tokenize the text into [ngrams](https://en.wikipedia.org/wiki/N-gram). For our analysis, a `gram` equals a whitespace delimited sequence of characters corresponding to an English word. The `N` in ngram corresponds to the number of  words considered to be part of the same unit. For example: a 1-gram is `new`, 2-gram is `new york`, 3-gram is `new york city` and a 4-gram is `new york city police`.

Finally, we build a document-by-term matrix by transforming the ngrams to a [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model). The columns in a docterm matrix each unique ngram. The row represent a document and the frequency that each ngram appears in the document. 

## Sampling the Dataset
The `4,269,678` lines from the complete dataset can be memory intensive for the text mining tools and slow the analysis. To speed things up, we subsample `1%` of the complete dataset and then work with the subsampled data for exploration and modeling. The subsampling implementation is in `Appendix 1`. 


```{r, echo=FALSE,cache=TRUE}
sample_vector_corpus <- get_sample_datums_vector_corpus()
content_stats_df <- do_explore_per_data_source(sample_vector_corpus)
```

```{r echo=FALSE, fig.align='center', fig.cap="Sampled Text Summary"}
kable(content_stats_df, row.names=FALSE, format = "markdown", caption="Table 2: Sampled Text Summary")
```

```{r, echo=FALSE, cache=TRUE}
docs <- load_sample_dircorpus()
docs <- preprocess_entries(docs)
all_corpus_grams <- get_docterm_matrix(docs, 1)
cover_stats = cover_percentage(all_corpus_grams$wf, instance_percent=0.50)
```

We see that the `mean` word frequency is nearly twice the `median` frequency for all data sources. The word frequency distribution has a [long tail](https://en.wikipedia.org/wiki/Long_tail) - as seen in the plot below. 
`r cover_stats$num_words` of `r cover_stats$total_words` (`r cover_stats$percent_of_total_words`%) words cover 50% of all word instances in the dataset. The `mean` word frequency is heavily weighted by a few words that occur very frequently.

```{r, echo=FALSE, cache=TRUE}
p <- all_docs_word_plot(sample_vector_corpus)
print(p)
```


### NGram Frequencies

Here are top bigrams.
```{r, echo=TRUE, cache=TRUE}
ngram_2 <- get_docterm_matrix(docs, 2)
p2 <- generate_word_frequency_plot(ngram_2$wf, "Top Bigrams for Sampled Text")
print(p2)
```

Here are top tri-grams
```{r, echo=TRUE, cache=TRUE}
ngram_3 <- get_docterm_matrix(docs, 3)
p3 <- generate_word_frequency_plot(ngram_3$wf, "Top Trigrams for Sampled Text")
print(p3)
```

Here are top 4-grams
```{r, echo=TRUE, cache=TRUE}
ngram_4 <- get_docterm_matrix(docs, 4)
p4 <- generate_word_frequency_plot(ngram_4$wf, "Top 4-grams for Sampled Text")
print(p4)
```

## Word Prediction using an NGrams

We build a tree using the ngrams and compute MLE () using the  [Dirichlet-multinomial model](path/to/wikipedia). We use
[node.tree](path/to/node.tree) which can build a tree from a data.frame. Now lets perform a search for "data".


### Word Prediction for: 'data'
```{r, echo=FALSE, cache=TRUE}
docs <- load_sample_dircorpus()
docs <- preprocess_entries(docs)
ngram_tree <- ngram_language_modeling(docs)
plot_tree_for_report(ngram_tree, title="Tree Lookup for: data")
```

Here are the maximum likelihood estimates. They show 6% likelihood that entry will be the next word: "data entry" has a frequency = 12 and "data" has a frequency of 198 - so the maximimum likelihood estimate is `r round((12/198)*100, digits=1)`%.

```{r, echo=TRUE, cache=TRUE}
results <- perform_search(ngram_tree, c("data"))
print(results)
```

### Word Prediction for: 'data entry'

Then if we query for "data entry", we search the tree the nodes "data" then "entry" and we will recommend the words "just" and "respond".

```{r, echo=FALSE, cache=TRUE}
plot_tree_for_report(ngram_tree, highlight_child = TRUE, title="Tree Lookup for: data entry")
```

```{r, echo=TRUE, cache=FALSE}
results <- perform_search(ngram_tree, c("data", "entry"))
print(results)
```

### Next Steps

- Build a model using the more than a 1% sample.
- Deploy the ngram tree to the server-side of an Shiny Application.


# Appendix

## Appendix 1: Subsampling Code

This code collects a 1% sample using a "coin flip" to decide which lines to choose.

```{r echo=TRUE,eval=FALSE}
# sample the datasci dir
sample_capstone_data <- function(fn, outfn, sample_len=0.01) {
  print(sprintf("Reading %s", fn))
  lines <- readLines(fn)
  set.seed(123)
  print(sprintf("Read %s Length %s", fn, length(lines)))
  lines_sample <- lines[rbinom(length(lines)*sample_len, length(lines), 0.5)]
  print(sprintf("Writing %s. Length %s", outfn, length(lines_sample)))
  write.csv(lines_sample, file=outfn, row.names=FALSE, col.names=FALSE)
}

sample_capstone_data("./data/final/en_US/en_US.twitter.txt",
                     "./data/final/en_US/sample/en_US.twitter.txt")
sample_capstone_data("./data/final/en_US/en_US.blogs.txt",
                     "./data/final/en_US/sample/en_US.blogs.txt")
sample_capstone_data("./data/final/en_US/en_US.news.txt",
                     "./data/final/en_US/sample/en_US.news.txt")
```



